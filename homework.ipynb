{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Zoom Camp 5 - Batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark session setup\n",
    "\n",
    "First of all, we need to setup the system.\n",
    "I will assume that the system already have Python and Java 17 installed.\n",
    "The setup commands can be executed just by running the `setup.sh` script.\n",
    "This will install Spark and download the required data files.\n",
    "\n",
    "Once the setup have been done, you can start tunning this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/02/23 17:07:13 WARN Utils: Your hostname, Baby resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/02/23 17:07:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/23 17:07:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"homework\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellow_df = spark.read.parquet(\"yellow_tripdata_2024-10.parquet\")\n",
    "yellow_df.repartition(4).write.parquet(\"yellow_tripdata_2024-10-repartitioned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 90M\n",
      "-rw-r--r-- 1 jcamps jcamps   0 Feb 23 17:07 _SUCCESS\n",
      "-rw-r--r-- 1 jcamps jcamps 23M Feb 23 17:07 part-00000-6724562b-525e-4cdf-af40-a96dea447378-c000.snappy.parquet\n",
      "-rw-r--r-- 1 jcamps jcamps 23M Feb 23 17:07 part-00001-6724562b-525e-4cdf-af40-a96dea447378-c000.snappy.parquet\n",
      "-rw-r--r-- 1 jcamps jcamps 23M Feb 23 17:07 part-00002-6724562b-525e-4cdf-af40-a96dea447378-c000.snappy.parquet\n",
      "-rw-r--r-- 1 jcamps jcamps 23M Feb 23 17:07 part-00003-6724562b-525e-4cdf-af40-a96dea447378-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh yellow_tripdata_2024-10-repartitioned.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125567"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "yellow_df.withColumn(\"date\", F.to_date(\"tpep_pickup_datetime\", \"d-M-y\")).filter(\n",
    "    (F.col(\"date\") == F.lit(\"2024-10-15\"))\n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162.61777777777777"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df.withColumn(\n",
    "    \"duration\", F.col(\"tpep_dropoff_datetime\") - F.col(\"tpep_pickup_datetime\")\n",
    ").agg(F.max(\"duration\")).collect()[0][0].total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth question\n",
    "\n",
    "This is a theory question. The port for the Spark dashboard is the 4040, as stated in the [documentation](https://spark.apache.org/docs/latest/configuration.html#spark-ui)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixth question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Zone|\n",
      "+--------------------+\n",
      "|Governor's Island...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"LocationID\", IntegerType()),\n",
    "        StructField(\"Borough\", StringType()),\n",
    "        StructField(\"Zone\", StringType()),\n",
    "        StructField(\"service_zone\", StringType()),\n",
    "    ]\n",
    ")\n",
    "zone_df = spark.read.schema(schema).option(\"header\", \"true\").csv(\"taxi_zone_lookup.csv\")\n",
    "count_trips_df = yellow_df.groupBy(\"PULocationID\").count().orderBy(\"count\")\n",
    "minimum_trips = count_trips_df.first()[\"count\"]\n",
    "count_trips_df.filter(F.col(\"count\") == minimum_trips).join(\n",
    "    zone_df, F.col(\"PULocationID\") == F.col(\"LocationID\")\n",
    ").select(\"Zone\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
